<div id="s:web:client:lesson" class="lesson">

  <p>
    Opening sockets, constructing HTTP requests, and parsing responses is tedious,
    so most people use libraries to do most of the work.
    Python comes with such a library called <code>urllib2</code>
    (because it's a replacement for an earlier library called <code>urllib</code>),
    but it exposes a lot of plumbing that most people never want to care about.
    Instead,
    we recommend using the Requests library.
    Here's an example that uses it to download a page from our web site:
  </p>

<pre>
import requests
response = requests.get("http://guide.software-carpentry.org/web/testpage.html")
print 'status code:', response.status_code
print 'content length:', response.headers['content-length']
print response.text
<span class="out">status code: 200
content length: 126
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Software Carpentry Test Page&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;p&gt;Use this page to test requests.&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</span>
</pre>

  <p class="continue">
    <code>request.get</code> does an HTTP GET on a URL
    and returns an object containing the response.
    That object's <code>status_code</code> member is the response's status code;
    its <code>content_length</code> member  is the number of bytes in the response data,
    and <code>text</code> is the actual data
    (in this case, an HTML page).
  </p>

  <div class="box">
    <h3>One at a Time</h3>
    <p class="fixme">no images etc. fetched</p>
  </div>

  <p>
    Sometimes a URL isn't enough on its own:
    for example,
    we have to specify what our search terms are
    if we are using a search engine.
    We could add these to the path in the URL,
    but that would be misleading
    (since most people think of paths as identifying files and directories),
    and we've have to decide whether <code>/software/carpentry</code>
    and <code>/carpentry/software</code> were the same search or not.
  </p>

  <p>
    What we should do instead is
    add parameters to the URL
    by adding a '?' to the URL
    followed by 'key=value' pairs separated by '&amp;'.
    For example,
    the URL <code>http://www.google.ca?q=Python</code>
    ask Google to search for pages related to Python&mdash;the key is the letter 'q',
    and the value is 'Python'&mdash;while
    the longer query
    <code>http://www.google.ca/search?q=Python&amp;client=Firefox</code>
    tells Google that we're using Firefox.
    We can pass whatever parameters we want,
    but it's up to the application running on the web site to decide
    which ones to pay attention to,
    and how to interpret them.
  </p>

  <div class="box">
    <h3>You Are Who You Say You Are</h3>
    <p>
      Yes,
      this means that we could write a program
      that tells websites it is Firefox,
      Internet Explorer,
      or pretty much anything else.
      We'll return to this and other security issues later.
    </p>
  </div>

  <p>
    Of course,
    if '?' and '&amp;' are special characters,
    there must be a way to escape them.
    The <a href="glossary.html#url-encoding">URL encoding</a> standard
    represents special characters using <code>"%"</code> followed by a 2-digit code,
    and replaces spaces with the '+' character
    (<a href="#f:url_encoding">Figure XXX</a>).
    Thus,
    to search Google for "grade&nbsp;=&nbsp;A+" (with the spaces),
    we would use the URL <code>http://www.google.ca/search?q=grade+%3D+A%2B</code>.
  </p>

  <figure id="f:url_encoding">
    <table>
      <tr>
        <th>Character</th>
        <th>Encoding</th>
      </tr>
      <tr>
        <td><code>"#"</code></td>
        <td><code>%23</code></td>
      </tr>
      <tr>
        <td><code>"$"</code></td>
        <td><code>%24</code></td>
      </tr>
      <tr>
        <td><code>"%"</code></td>
        <td><code>%25</code></td>
      </tr>
      <tr>
        <td><code>"&amp;"</code></td>
        <td><code>%26</code></td>
      </tr>
      <tr>
        <td><code>"+"</code></td>
        <td><code>%2B</code></td>
      </tr>
      <tr>
        <td><code>","</code></td>
        <td><code>%2C</code></td>
      </tr>
      <tr>
        <td><code>"/"</code></td>
        <td><code>%2F</code></td>
      </tr>
      <tr>
        <td><code>":"</code></td>
        <td><code>%3A</code></td>
      </tr>
      <tr>
        <td><code>";"</code></td>
        <td><code>%3B</code></td>
      </tr>
      <tr>
        <td><code>"="</code></td>
        <td><code>%3D</code></td>
      </tr>
      <tr>
        <td><code>"?"</code></td>
        <td><code>%3F</code></td>
      </tr>
      <tr>
        <td><code>"@"</code></td>
        <td><code>%40</code></td>
      </tr>
    </table>
    <figcaption>Figure XXX: URL Encoding</figcaption>
  </figure>

  <p>
    Encoding things by hand is very error-prone,
    so the Requests library lets us use
    a dictionary of key-value pairs instead
    via the keyword argument <code>params</code>:
  </p>

<pre>
import requests
parameters = {'q' : 'Python', 'client' : 'Firefox'}
response = requests.get('http://www.google.com/search', params=parameters)
print 'actual URL:', response.url
<span class="out">actual URL: http://www.google.com/search?q=Python&amp;client=Firefox</span>
</pre>

  <p class="continue">
    You should <em>always</em> let the library build the URL for you,
    rather than doing it yourself:
    there are subtleties we haven't covered,
    and even if there weren't,
    there's no point duplicating code that's already been written and tested.
  </p>

  <p>
    Suppose we want to write a script that actually <em>does</em> search Google.
    Constructing a URL is easy.
    Sending it and reading the response is easy too,
    but parsing the response is hard,
    since there's a lot of stuff in the page that Google sends back.
    Many first-generation web applications relied on
    <a href="glossary.html#screen-scraping">screen scraping</a>
    to get data,
    i.e.,
    they would search for substrings in the HTML
    using something like <a href="#a:beautiful-soup">Beautiful Soup</a>.
    They had to do this because a lot of hand-written HTML was improperly formatted:
    for example,
    it was quite common to use <code>&lt;br&gt;</code> on its own to break a line.
  </p>

  <p>
    Screen scraping is always hard to get right if the page layout is complex.
    It is also fragile:
    whenever the layout of the pages changes,
    the application will most likely break
    because data is no longer where it was.
  </p>

  <p>
    Most modern web applications try to sidestep this problem
    by providing some sort of <a href="glossary.html#web-services">web services</a> interface,
    which is a lot simpler than it sounds.
    When a client sends a request,
    it indicates that it wants machine-oriented data rather than human-readable HTML
    by using a slightly different URL
    (<a href="#f:web_services">Figure XXX</a>).
    When asked for data,
    the server sends back <a href="setdict.html#s:json">JSON</a>,
    XML,
    or something else that is easy for a program to handle.
    If the client asks for HTML,
    on the other hand,
    the application turns that data into HTML pages with italics and colored highlights and the like
    to make it easy for human beings to read.
  </p>

  <figure id="f:web_services">
    <img src="web/web_services.png" alt="Web Services"/>
    <figcaption>Figure XXX: Web Services</figcaption>
  </figure>

  <p>
    Using "live" data from a web service is a powerful way to get a lot of science done in a hurry,
    but only when it works.
    As a case in point,
    we wanted to use bird-watching data from <a href="http://ebird.org">ebird.org</a> in this example,
    but their server was locked down for security reasons
    when it came time for us to write our examples.
    (This is another way in which software is like other experimental apparatus:
    odds are that when you need it most,
    it will be broken or someone will have borrowed it.)
    We therefore chose to use climate data from the World Bank instead.
    According to <a href="http://data.worldbank.org/developers/climate-data-api">the documentation</a>,
    data for a particular country can be found at:
  </p>

<pre>
http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/<em>VARIABLE</em>/year/<em>ISO</em>.<em>FORMAT</em>
</pre>

  <p class="continue">
    where:
  </p>

  <ul>
    <li>
      <em>VARIABLE</em> is either "pr" (for precipitation)
      or "tas" (for <em>t</em>emperature <em>a</em>t <em>s</em>urface);
    </li>
    <li>
      <em>ISO</em> is the International Standards Organization's 3-letter country code
      for the country of interest,
      and
    </li>
    <li>
      <em>FORMAT</em> is "JSON" for JSON,
      and other strings for other formats.
    </li>
  </ul>

  <p>
    Let's try getting temperature data for France:
  </p>

<pre id="a:sample-response">
&gt;&gt;&gt; <span class="in">import requests</span>
&gt;&gt;&gt; <span class="in">url = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/FRA.JSON'</span>
&gt;&gt;&gt; <span class="in">response = requests.get(url)</span>
&gt;&gt;&gt; <span class="in">print response.text</span>
<span class="out">[{"year":1901, "data":9.748865},
 {"year":1902, "data":9.864603},
 {"year":1903, "data":10.130159},
 ...
 {"year":2009,"data":11.709985}]</span>
</pre>

  <p>
    This is straightforward to interpret:
    the outer list element contains a dictionary for each year,
    each of which contains <code>"year"</code> and <code>"data"</code> entries.
    Let's use this to write a program
    that compares the data for two countries
    (which is the problem Carla wanted to solve at the start of this chapter).
    We need to know which countries to compare:
  </p>

<pre>
def main(args):
    first_country = 'AUS'
    second_country = 'CAN'
    if len(args) &gt; 0:
        first_country = args[0]
    if len(args) &gt; 1:
        second_country = args[1]

    result = ratios(first_country, second_country)
    display(result)

def ratios(first, second):
    '''Calculate ratio of average temperatures for two countries over time.'''
    return {} <span class="comment"># FIXME: fill in</span>

def display(values):
    '''Show dictionary entries in sorted order.'''
    keys = values.keys()
    keys.sort()
    for k in keys:
        print k, values[k]

if __name__ == '__main__':
    main(sys.argv[1:])
</pre>

  <p>
    The pattern here should be familiar:
    we solve the top-level problem as if we already have the functions we need,
    then come back and fill them in.
    In this case,
    this function to be filled in is <code>ratios</code>,
    which fetches data and calculates our result:
  </p>

<pre src="web/temperatures.py">
def ratios(first, second):
    '''Calculate ratio of average temperatures for two countries over time.'''
    first = get_temps(first)
    second = get_temps(second)
    assert len(first) == len(second), 'Length mis-match in results'
    result = {}
    for (i, first_entry) in enumerate(first):
        year = first_entry['year']
        second_entry = second[i]
        assert second_entry['year'] == year, 'Year mis-match'
        result[year] = first_entry['data'] / second_entry['data']
    return result
</pre>

  <p>
    It depends in turn on <code>get_temps</code>:
  </p>

<pre src="web/temperatures.py">
URL = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/%s.JSON'

...all the code written so far...

def get_temps(country_code):
    '''Get annual temperatures for a country.'''
    response = requests.get(URL % country_code)
    assert response.status_code == 200, \
           'Failed to get data for %s' % country_code
    return json.loads(response.text)
</pre>

  <p>
    But wait a second:
    judging from the <a href="#a:sample-response">sample response shown earlier</a>,
    temperatures are being reported in Celsius.
    We should probably convert them to Kelvin
    to make the ratios more meaningful
    (and to avoid the risk of dividing by zero).
    Let's modify <code>get_temps</code>:
  </p>

<pre src="web/temperatures.py">
def get_temps(country_code):
    '''Get annual temperatures for a country.'''
    response = requests.get(URL % country_code)
    assert response.status_code == 200, \
           'Failed to get data for %s' % country_code
    <span class="highlight">result = json.loads(response.text)
    for entry in result:
        result['data'] = kelvin(result['data'])
    return result</span>
</pre>

  <p class="continue">
    and add the required conversion function:
  </p>

<pre src="web/temperatures.py">
def kelvin(celsius):
    '''Convert degrees C to degrees K.'''
    return celsius + 273.15
</pre>

  <p>
    Let's try running this program with no arguments
    to compare Australia to Canada:
  </p>

<pre>
$ <span class="in">python temperatures.py</span>
<span class="out">1901 1.10934799048
1902 1.11023963325
1903 1.10876094164
...  ...
2007 1.10725265753
2008 1.10793365185
2009 1.10865537105</span>
</pre>

  <p class="continue">
    and then with arguments to compare Malaysia to Norway:
  </p>

<pre>
$ <span class="in">python temperatures.py MYS NOR</span>
<span class="out">1901 1.08900632708
1902 1.09536126502
1903 1.08935268463
...  ...
2007 1.08564675748
2008 1.08481881663
2009 1.08720464013</span>
</pre>

  <p>
    Only six lines in this program do anything webbish
    (i.e., format the actual URL and get the data).
    The remaining 47 lines are the user interface
    (handling command-line arguments and printing output)
    data manipulation
    (converting temperatures and calculating ratios),
    <code>import</code> statements,
    and docstrings.
    It really is that simple.
  </p>

</div>
