<div id="s:web:syndicate:lesson" class="lesson">

  <p>
    We'll now use what we have learned to build a simple tool
    to download new temperature comparisons from a web site.
    In broad strokes,
    our program will keep a list of URLs to download data from,
    along with a timestamp showing when data was last downloaded.
    When we run the program,
    it will poll each site to see if any new data sets have been added
    since the last check.
    If any have,
    the program will display their URLs.
  </p>

  <p>
    In order for this to work,
    each of the sites that's providing data needs to be able to tell us
    what data sets it has calculated,
    and when they were created.
    This information is in the site's <code>index.html</code> file in human-readable form,
    but it's also in the <code>index.json</code> file each site is maintaining.
    Client programs can load this file directly without having to do any parsing,
    so we'll rely on that.
  </p>

  <div class="box">
    <h3>Making Life Simpler</h3>
    <p>
      An earlier version of this tutorial loaded the HTML version of the index
      and extracted dates and URLs from it.
      Doing so only required twelve extra lines of code&mdash;but
      an extra 1200 words to explain how to read HTML into a program
      and find things in it.
      Storing information in machine-friendly formats for machines to use
      makes life a <em>lot</em> simpler...
    </p>
  </div>

  <p>
    The next step is to decide how to keep track of what we have downloaded and when.
    The simplest thing is to create another JSON file
    containing the timestamp and the list of URLs.
    We'll call this <code>sources.json</code>:
  </p>

<pre>
{
    "timestamp" : "2013-05-02:07:04:03",
    "sites" : [
        "http://software-carpentry.org/temperatures/index.json",
        "http://some.other.site/some/path/index.json"
    ]
}
</pre>

  <p class="continue">
    (Again, a larger application would use a database of some kind,
    but that's more than we need right now.)
    Each time we run our program,
    it will read this file,
    then download each <code>index.json</code> file.
    If any of those files contain links to data sets that are newer than the timestamp,
    it will print the data set's URL.
    (A real data analysis program would download the data and do something with it.)
    We will then save a fresh copy of <code>sources.json</code>
    with an updated timestamp
    (<a href="#f:syndication_lifecycle">Figure XXX</a>).
    Our main program looks like this:
  </p>

<pre src="web/syndicate.py">
import date

def main(sources_path):
    '''Check all data sites in list, then update timestamp of sources.json.'''
    old_timestamp, all_sources = read_sources(sources_path)
    new_timestamp = date.datetime.now()
    for source in all_sources:
        for url in get_new_datasets(old_timestamp, source):
            process(url)
    write_sources(sources_path, new_timestamp, sources)
</pre>

  <figure id="f:syndication_lifecycle">
    <img src="web/syndication_lifecycle.png" alt="Syndication Lifecycle" />
    <figcaption>Figure XXX: Syndication Lifecycle</figcaption>
  </figure>

  <p>
    That seems pretty simple;
    the only subtlety is that we calculate the new timestamp
    <em>before</em> we start checking for new datasets.
    The reason is that this check might take
    anything from a few seconds to a few hours,
    depending on how busy the Internet is
    and how much data we actually download.
    If we wait until we're done
    and then record that moment as the new timestamp,
    then the next time we run our program,
    we won't download any datasets that were created
    between the time we started the first run of our program
    and the time it finished
    (<a href="#f:when_to_timestamp">Figure XXX</a>).
  </p>

  <figure id="f:when_to_timestamp">
    <img src="web/when_to_timestamp.png" alt="When to Create Timestamps"/>
    <figcaption>Figure XXX: When to create Timestamps</figcaption>
  </figure>

  <p>
    We now have four functions to write:
    <code>read_sources</code>,
    <code>write_sources</code>,
    <code>get_new_datasets</code>,
    and
    <code>process</code>.
    Reading and writing the <code>sources.json</code> file is pretty simple:
  </p>

<pre>
import json

def read_sources(path):
    '''Read timestamp and data sources from JSON files.'''
    reader = open(path, 'r')
    data = json.load(reader)
    timestamp = data['timestamp']
    sources = data['sources']
    return timestamp, sources

def write_sources(sources_path, timestamp, sources):
    '''Write timestamp and data sources to JSON file.'''
    data = {'timestamp' : timestamp,
            'sources'   : sources}
    writer = open(sources_path, 'w')
    json.dump(data, writer)
    writer.close()
</pre>

  <p>
    What about processing a URL?
    Right now,
    we're just going to print it,
    though in a real application we would probably download the data
    and do some further calculations with it:
  </p>

<pre>
def process(url):
    '''Placeholder for processing a data set given its URL.'''
    print url
</pre>

  <p>
    Finally,
    we need to construct a list of dataset URLs
    given the URL of an <code>index.json</code> file:
  </p>

<pre>
import requests

def get_new_datasets(last_checked, index_url):
    '''Return a list of URLs of datasets that are newer than the timestamp.'''
    response = requests.get(index_url)
    index_data = json.loads(index.text)
    result = []
    for (country_a, country_b, updated) in index_data:
        dataset_timestamp = datetime.parse(updated)
        if dataset_timestamp &gt;= last_checked:
            dataset_url = make_dataset_url(index_url, country_a, country_b)
            result.append(dataset_url)
    return result
</pre>

  <p>
    The logic here is straightforward:
    grab the <code>index.json</code> file,
    check each dataset to see if it's newer than the last time we checked,
    and if it is&mdash;hm.
    This code uses a not-yet-written function called <code>make_dataset_url</code>
    to construct the URL for the specific dataset
    from the URL of the index file
    and the two country codes,
    but as we discussed <a href="#b:index:explicit">earlier</a>,
    asking client programs to construct links themselves is a bad idea.
    Instead,
    we should modify the <code>index.json</code> files so that they include the URLs.
    Doing this is left as an exercise for the reader.
  </p>

  <p>
    But hang on:
    what exactly are we downloading when we download data sets?
    Right now,
    our temperature ratio files are all HTML pages;
    if we want to use that information in programs,
    it would be a lot easier if producers generated JSON files
    that consumers could use directly.
    It's almost trivial to extend our original program
    to produce such a file
    each time it produces a new HTML file,
    and to include the URLs for both files in both versions of the index
    (<a href="#f:final_system">Figure XXX</a>).
    Once we've done that,
    we have a first-class data syndication system:
    human-friendly and machine-friendly formats live side by side,
    so scientists and programs all over the world
    can make use of our results as soon as they appear.
  </p>

  <figure id="f:final_system">
    <img src="web/final_system.png" alt="Final System" />
    <figcaption>Figure XXX: Final System</figcaption>
  </figure>

</div>
