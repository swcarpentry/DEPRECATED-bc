{
 "metadata": {
  "name": "",
  "signature": "sha256:3e37bd571efbb4c2ff7682403e8c07139d68cba7b08b0c96cac0a154065b5eee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data management"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our previous lessons have shown us how to write programs that ingest a list of data files, perform some calculations on those data, and then print a final result to the screen. While this was a useful exercise in learning the principles of scripting and parsing the command line, in most cases the output of our programs will not be so simple. Instead, programs typically take data as input, manipulate that data, and then output yet more data. Over the course of a multi-year research project, most reseachers will write many different programs that produce many different output datasets. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to:\n",
      "\n",
      "* Manage our data in such as way as to avoid confusion/calamity  \n",
      "\n",
      "Along the way, we will learn:\n",
      "\n",
      "* how to create a Data Reference Syntax\n",
      "* how to view the contents of binary files\n",
      "* about data provenance and metadata"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What's in a name?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this lesson we are going to process some of the climate model data that was submitted to the [CMIP5](http://cmip-pcmdi.llnl.gov/cmip5/) project. This project informed many of the results presented in the Intergovernmental Panel on Climate Change (IPCC) [Fifth Assessment Report](http://www.ipcc.ch/report/ar5/wg1/), making it one of the most widely used datasets in the world.\n",
      "\n",
      "First off, let's see what files we've got:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls *.nc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc  vas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc\r\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing to notice is the distinctive [Data Reference Syntax](http://cmip-pcmdi.llnl.gov/cmip5/docs/cmip5_data_reference_syntax.pdf) (DRS) associated with CMIP5. Modelling groups contributing to the project must name their files according to the following structure:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`<variable name>_<MIP table>_<model>_<experiment>_<ensemble member>_<temporal subset>_<geographical info>.nc`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this we can deduce, without even inspecting the contents of the file, that we have surface zonal (i.e. east-west; abbreviated `uas`) and meridional (i.e. north-south; abbreviated `vas`) wind speed data. It belongs to the atmospheric, monthly timescale data group (`Amon`) and was derived from an Australian model known as `ACCESS1-3`. The external forcing applied to the model was that corresponding to the `rcp85` scenario (high future greenhouse gas emissions), it was the `r1i1p1`'th realisation of the model, and we have the data for June, July and August of the year 2050 for the Australian (`aus`) region.\n",
      "\n",
      "The DRS for CMIP5 actually goes further than just the file name. If you download a whole heap of CMIP5 data, it comes with the following directory structure:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`/<activity>/<product>/<institute>/<model>/<experiment>/<frequency>/<modeling realm>/<variable name>/<ensemble member>/`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the first instance this level of detail seems like a bit of overkill, but consider the scope of the CMIP5 data archive. It contains data from over 50 climate models for upwards of 100 different variables and 50 or so different experiments, for which each modelling group typically provides bewteen 3 and 10 different realisations. Since the data are so well labelled, calculating the average surface temperature (`tas`) across the `r1i1p1` realisation of all models that provided monthly timescale data for the `rcp85` scenario can be achieved with a single [cdo](https://code.zmaw.de/embedded/cdo/1.6.1/cdo.html) bash shell command like the following, which is truly amazing:      "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`cdo ensmean /*/*/*/*/rcp85/mon/*/tas/r1i1p1/tas_Amon_*_rcp85_r1i1p1_*.nc` "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unless your research involves analysing CMIP5 data, you may never deal with such a large dataset. Nevertheless, it is a very good idea to develop your own personal DRS for the data that you do have. This often involves investing some time at the beginning of a project to think carefully about the design of your directory and file name structures (as these can be very hard to change later on). The combination of bash shell wildcards and a well planned DRS is one of the easiest ways to make your research more efficient and reliable.   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Challenge\n",
      "\n",
      "We haven't even looked inside our CMIP5 data files and already we have the beginnings of a detailed data management plan. The first step in any research project should be to develop such a plan, so for this challenge we are going to turn back time. If you could start your current research project all over again, what would your data management plan look like? Things to consider include:  \n",
      "\n",
      "* Data Reference Syntax\n",
      "* How long it will take to obtain the data\n",
      "* Storage and backup (here's a [post](http://drclimate.wordpress.com/2013/04/16/backing-up-your-work/) with some backup ideas)\n",
      "\n",
      "Write down and discuss your plan with your partner.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Binary file formats"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've identified our CMIP5 files, let's go ahead and look what's inside. Our initial impulse might be to enter \n",
      "\n",
      "    !cat uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc\n",
      "\n",
      "but in this case such a command will produce an incomprehensible mix symbols and letters. The reason is that up until now, we have been dealing with [text files](http://en.wikipedia.org/wiki/Text_file). These consist of a simple sequence of character data (represented using ASCII, Unicode, or some other standard) separated into lines, meaning that text files are human-readable when opened with a text editor or displayed using `cat`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "All other file types are known collectively as [binary files](http://en.wikipedia.org/wiki/Binary_file). They tend to be smaller and faster for the computer to interpret than text files, but the payoff is that they aren't human-readable unless you have the right intpreter (e.g. `.doc` files aren't readable with your text editor and must instead be opened with Microsoft Word). In this case we have a Network Common Data Form (netCDF) file, so we need to use a special command line utility called `ncdump` to view the contents.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ncdump -h uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "netcdf uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus {\r\n",
        "dimensions:\r\n",
        "\tlon = 26 ;\r\n",
        "\tnb2 = 2 ;\r\n",
        "\tlat = 28 ;\r\n",
        "\ttime = UNLIMITED ; // (3 currently)\r\n",
        "variables:\r\n",
        "\tdouble lon(lon) ;\r\n",
        "\t\tlon:standard_name = \"longitude\" ;\r\n",
        "\t\tlon:long_name = \"longitude\" ;\r\n",
        "\t\tlon:units = \"degrees_east\" ;\r\n",
        "\t\tlon:axis = \"X\" ;\r\n",
        "\t\tlon:bounds = \"lon_bnds\" ;\r\n",
        "\tdouble lon_bnds(lon, nb2) ;\r\n",
        "\tdouble lat(lat) ;\r\n",
        "\t\tlat:standard_name = \"latitude\" ;\r\n",
        "\t\tlat:long_name = \"latitude\" ;\r\n",
        "\t\tlat:units = \"degrees_north\" ;\r\n",
        "\t\tlat:axis = \"Y\" ;\r\n",
        "\t\tlat:bounds = \"lat_bnds\" ;\r\n",
        "\tdouble lat_bnds(lat, nb2) ;\r\n",
        "\tdouble time(time) ;\r\n",
        "\t\ttime:standard_name = \"time\" ;\r\n",
        "\t\ttime:bounds = \"time_bnds\" ;\r\n",
        "\t\ttime:units = \"days since 1-01-01 00:00:00\" ;\r\n",
        "\t\ttime:calendar = \"proleptic_gregorian\" ;\r\n",
        "\tdouble time_bnds(time, nb2) ;\r\n",
        "\t\ttime_bnds:units = \"days since 1-01-01 00:00:00\" ;\r\n",
        "\t\ttime_bnds:calendar = \"proleptic_gregorian\" ;\r\n",
        "\tfloat uas(time, lat, lon) ;\r\n",
        "\t\tuas:standard_name = \"eastward_wind\" ;\r\n",
        "\t\tuas:long_name = \"Eastward Near-Surface Wind\" ;\r\n",
        "\t\tuas:units = \"m s-1\" ;\r\n",
        "\t\tuas:_FillValue = 1.e+20f ;\r\n",
        "\t\tuas:cell_methods = \"time: mean\" ;\r\n",
        "\t\tuas:history = \"2012-03-14T04:40:42Z altered by CMOR: Treated scalar dimension: \\'height\\'. 2012-03-14T04:40:42Z altered by CMOR: replaced missing value flag (-1.07374e+09) with standard missing value (1e+20).\" ;\r\n",
        "\t\tuas:associated_files = \"baseURL: http://cmip-pcmdi.llnl.gov/CMIP5/dataLocation gridspecFile: gridspec_atmos_fx_ACCESS1-3_rcp85_r0i0p0.nc\" ;\r\n",
        "\r\n",
        "// global attributes:\r\n",
        "\t\t:CDI = \"Climate Data Interface version 1.5.6 (http://code.zmaw.de/projects/cdi)\" ;\r\n",
        "\t\t:Conventions = \"CF-1.4\" ;\r\n",
        "\t\t:history = \"Thu Nov 07 14:19:44 2013: cdo sellonlatbox,110,160,-45,-10 uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc\\n\",\r\n",
        "\t\t\t\"Thu Nov 07 14:13:51 2013: cdo seldate,2050-06-01,2050-08-31 uas_Amon_ACCESS1-3_rcp85_r1i1p1_200601-210012.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc\\n\",\r\n",
        "\t\t\t\"CMIP5 compliant file produced from raw ACCESS model output using the ACCESS Post-Processor and CMOR2. 2012-03-14T04:40:43Z CMOR rewrote data to comply with CF standards and CMIP5 requirements. Fri Apr 13 12:32:01 2012: corrected model_id from ACCESS1-3 to ACCESS1.3 Fri Apr 13 14:07:50 2012: forcing attribute modified to correct value Wed May  2 13:39:09 2012: updated version number to v20120413.\" ;\r\n",
        "\t\t:institution = \"CSIRO (Commonwealth Scientific and Industrial Research Organisation, Australia), and BOM (Bureau of Meteorology, Australia)\" ;\r\n",
        "\t\t:institute_id = \"CSIRO-BOM\" ;\r\n",
        "\t\t:experiment_id = \"rcp85\" ;\r\n",
        "\t\t:model_id = \"ACCESS1.3\" ;\r\n",
        "\t\t:forcing = \"GHG, Oz, SA, Sl, Vl, BC, OC, (GHG = CO2, N2O, CH4, CFC11, CFC12, CFC113, HCFC22, HFC125, HFC134a)\" ;\r\n",
        "\t\t:parent_experiment_id = \"historical\" ;\r\n",
        "\t\t:parent_experiment_rip = \"r1i1p1\" ;\r\n",
        "\t\t:branch_time = 732311. ;\r\n",
        "\t\t:contact = \"The ACCESS wiki: http://wiki.csiro.au/confluence/display/ACCESS/Home. Contact Tony.Hirst@csiro.au regarding the ACCESS coupled climate model. Contact Peter.Uhe@csiro.au regarding ACCESS coupled climate model CMIP5 datasets.\" ;\r\n",
        "\t\t:references = \"See http://wiki.csiro.au/confluence/display/ACCESS/ACCESS+Publications\" ;\r\n",
        "\t\t:initialization_method = 1 ;\r\n",
        "\t\t:physics_version = 1 ;\r\n",
        "\t\t:tracking_id = \"724f536a-c5fa-4a68-85f1-ff277af34c75\" ;\r\n",
        "\t\t:version_number = \"v20120413\" ;\r\n",
        "\t\t:product = \"output\" ;\r\n",
        "\t\t:experiment = \"RCP8.5\" ;\r\n",
        "\t\t:frequency = \"mon\" ;\r\n",
        "\t\t:creation_date = \"2012-03-14T04:40:43Z\" ;\r\n",
        "\t\t:project_id = \"CMIP5\" ;\r\n",
        "\t\t:table_id = \"Table Amon (01 February 2012) 01388cb4507c2f05326b711b09604e7e\" ;\r\n",
        "\t\t:title = \"ACCESS1-3 model output prepared for CMIP5 RCP8.5\" ;\r\n",
        "\t\t:parent_experiment = \"historical\" ;\r\n",
        "\t\t:modeling_realm = \"atmos\" ;\r\n",
        "\t\t:realization = 1 ;\r\n",
        "\t\t:cmor_version = \"2.8.0\" ;\r\n",
        "\t\t:CDO = \"Climate Data Operators version 1.5.6.1 (http://code.zmaw.de/projects/cdo)\" ;\r\n",
        "}\r\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By using the `-h` flag, only the header of the file has been shown. The great thing about netCDF files is that the header contains [metadata](http://en.wikipedia.org/wiki/Metadata) - that is, data about the data. Each variable has it's own 'variable attributes' (e.g. the `lat` axis has a `long_name` and `units` attribute) and then there are also a whole suite of global attributes that describe the history of the file. When we write out own netCDF output later on, we will discuss the conventions around netCDF metadata in more detail. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Calculating the wind speed"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To read in our data, we are going to use a library known as the Climate Data Management System (`cdms2`). This library is part of a larger open-source software package called [Ultrascale Visualisation - Climate Data Analysis Tools](http://uvcdat.llnl.gov/) (UV-CDAT).  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cdms2\n",
      "\n",
      "u_name = 'uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc'\n",
      "u_file = cdms2.open(u_name)\n",
      "u_data = u_file('uas')\n",
      "\n",
      "v_name = 'vas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc'\n",
      "v_file = cdms2.open(v_name)\n",
      "v_data = v_file('vas')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our two variables, `udata` and `vdata`, are cdms2 transient variables. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'udata is of type:', type(u_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "udata is of type: <class 'cdms2.tvariable.TransientVariable'>\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The nice thing about these transient variables is that they carry the netCDF metadata with them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Metadata about the time axis:' \n",
      "print u_data.getTime()\n",
      "\n",
      "print 'Raw time axis values:'\n",
      "print u_data.getTime()[:]\n",
      "\n",
      "print 'Time axis values in a friendlier format:'\n",
      "print u_data.getTime().asComponentTime()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Metadata about the time axis:\n",
        "   id: time\n",
        "   Designated a time axis.\n",
        "   units:  days since 1-01-01 00:00:00\n",
        "   Length: 3\n",
        "   First:  748548.0\n",
        "   Last:   748609.5\n",
        "   Other axis attributes:\n",
        "      standard_name: time\n",
        "      calendar: proleptic_gregorian\n",
        "      axis: T\n",
        "   Python id:  0x3599190\n",
        "\n",
        "Raw time axis values:\n",
        "[ 748548.   748578.5  748609.5]\n",
        "Time axis values in a friendlier format:\n",
        "[2050-6-16 0:0:0.0, 2050-7-16 12:0:0.0, 2050-8-16 12:0:0.0]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now go ahead and calculate the wind speed,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsp_data = (u_data**2 + v_data**2)**0.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and our transient variables are smart enough to pass along the relevant metadata to our new variable:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Metadata about the time axis:' \n",
      "print wsp_data.getTime()\n",
      "\n",
      "print 'Time axis values in a friendlier format:'\n",
      "print wsp_data.getTime().asComponentTime()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Metadata about the time axis:\n",
        "   id: time\n",
        "   Designated a time axis.\n",
        "   units:  days since 1-01-01 00:00:00\n",
        "   Length: 3\n",
        "   First:  748548.0\n",
        "   Last:   748609.5\n",
        "   Other axis attributes:\n",
        "      standard_name: time\n",
        "      calendar: proleptic_gregorian\n",
        "      axis: T\n",
        "   Python id:  0x321db50\n",
        "\n",
        "Time axis values in a friendlier format:\n",
        "[2050-6-16 0:0:0.0, 2050-7-16 12:0:0.0, 2050-8-16 12:0:0.0]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> ### Climate and Forecast (CF) metadata convention\n",
      ">\n",
      "> It's incredibly useful that libraries like `cdms2` can make use of the metadata stored in netCDF files to create methods like `asComponentTime()`. However, let's put ourselves in the shoes of the developers of `cdms2` for a minute. In order to convert the time axis to a meaningful list of dates, the library needs to first identify the units of the time axis. This isn't as easy as you'd think, since the creator of the netCDF file could easily have called the `units` attribute \"`measure`,\" or \"`scale`,\" or something else completely unpredictable instead. They could also have defined the units as \"`weeks since 1-01-01 00:00:00`,\" or \"`milliseconds after 1979-12-31`.\" Obviously what is needed is a standard method for defining netCDF attributes, and that\u2019s where the [Climate and Forecast (CF) metadata convention](http://cf-pcmdi.llnl.gov/) comes in.\n",
      "\n",
      "> The CF metadata standard was first defined back in the early 2000s and has now been adopted by all the major institutions and projects in the weather/climate sciences. There is a nice [blog post](http://drclimate.wordpress.com/2014/06/09/are-you-cf-compliant/) on the topic if you'd like more information, but for the most part you just need to be aware that if a tool you're using isn't working, if might be because your netCDF file isn't CF compliant."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data provenance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we go ahead and create a new script (`calc_wind_speed.py`) for calculating the wind speed, there's just one more thing to consider. Looking closely at the global attributes of `uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc` you can see that the entire history of the file, all the way back to its initial download, has been recorded in the `history` attribute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "global_atts = u_file.attributes\n",
      "old_history = global_atts['history']\n",
      "print old_history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Thu Nov 07 14:19:44 2013: cdo sellonlatbox,110,160,-45,-10 uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc\n",
        "Thu Nov 07 14:13:51 2013: cdo seldate,2050-06-01,2050-08-31 uas_Amon_ACCESS1-3_rcp85_r1i1p1_200601-210012.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc\n",
        "CMIP5 compliant file produced from raw ACCESS model output using the ACCESS Post-Processor and CMOR2. 2012-03-14T04:40:43Z CMOR rewrote data to comply with CF standards and CMIP5 requirements. Fri Apr 13 12:32:01 2012: corrected model_id from ACCESS1-3 to ACCESS1.3 Fri Apr 13 14:07:50 2012: forcing attribute modified to correct value Wed May  2 13:39:09 2012: updated version number to v20120413.\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last two entries, for instance, were generated by the `cdo` package when it was used to select a temporal (`seldate`) and spatial (`sellonlatbox`) subset of the original data file. This practice of recording the history of the file ensures the [provenance](http://en.wikipedia.org/wiki/Provenance) of the data. In other words, a complete record of everything that has been done to the data is stored with the data, which avoids any confusion in the event that the data is ever moved, passed around to different users, or viewed by its creator many months later.  \n",
      "\n",
      "If we want to create our own entry for the history attribute, we'll need to be able to create a: \n",
      "\n",
      "* Time stamp\n",
      "* Record of what was entered at the command line in order to execute `calc_wind_speed.py`\n",
      "* Method of indicating which verion of the script was run (i.e. because the script is in our git repository)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Time stamp"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A library called `datetime` can be used to find out the time and date right now:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "\n",
      "time_stamp = datetime.datetime.now().strftime(\"%a %b %d %H:%M:%S %Y\")\n",
      "print time_stamp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tue Jun 10 16:59:58 2014\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `strftime` function can be used to customise the appearance of a datetime object; in this case we've made it look just like the other time stamps in our data files. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Command line record"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the Software Carpentry [lesson on command line programs](http://www.software-carpentry.org/v5/novice/python/06-cmdline.html) we met `sys.argv`, which contains all the arguments entered by the user at the command line:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "print sys.argv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['-c', '-f', '/home/dbirving/.ipython/profile_default/security/kernel-0f9c6be1-a1d7-4bd9-8f12-c7772ab46070.json', \"--IPKernelApp.parent_appname='ipython-notebook'\", '--profile-dir', '/home/dbirving/.ipython/profile_default', '--parent=1']\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In launching this IPython notebook, you can see that a number of command line arguments were used. To join all these list elements up, we can use the `join` function that belongs to Python strings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "args = \" \".join(sys.argv)\n",
      "print args"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-c -f /home/dbirving/.ipython/profile_default/security/kernel-0f9c6be1-a1d7-4bd9-8f12-c7772ab46070.json --IPKernelApp.parent_appname='ipython-notebook' --profile-dir /home/dbirving/.ipython/profile_default --parent=1\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While this list of arguments is very useful, it doesn't tell us which Python installation was used to execute those arguments. The `sys` library can help us out here too:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exe = sys.executable\n",
      "print exe "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/local/uvcdat/1.5.1/bin/python\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Git hash"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the Software Carpentry [lessons on git](http://www.software-carpentry.org/v5/novice/git/index.html) we learned that each commit is associated with a unique 40-character identifier known as a hash. We can use the git Python library to get the hash associated with the script:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from git import Repo\n",
      "import os\n",
      "\n",
      "git_hash = Repo(os.getcwd()).head.commit.hexsha\n",
      "print git_hash"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2db1f92b517fe4262f4c8d98d2ec8b9b4c89b154\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now put all this information together for our history entry: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entry = \"\"\"%s: %s %s (Git hash: %s)\"\"\" %(time_stamp, exe, args, git_hash[0:7])\n",
      "print entry"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tue Jun 10 16:59:58 2014: /usr/local/uvcdat/1.5.1/bin/python -c -f /home/dbirving/.ipython/profile_default/security/kernel-0f9c6be1-a1d7-4bd9-8f12-c7772ab46070.json --IPKernelApp.parent_appname='ipython-notebook' --profile-dir /home/dbirving/.ipython/profile_default --parent=1 (Git hash: 2db1f92)\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Putting it all together"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we've been experimenting in the IPython notebook to familiarise ourselves with UV-CDAT and the other Python modules that might be useful for calculating the wind speed. We should now go ahead and write a script, so we can repeat the process with a single entry at the command line:  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat calc_wind_speed.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "import os, sys\r\n",
        "import datetime\r\n",
        "from git import Repo\r\n",
        "\r\n",
        "import cdms2\r\n",
        "cdms2.setNetcdfShuffleFlag(0)\r\n",
        "cdms2.setNetcdfDeflateFlag(0)\r\n",
        "cdms2.setNetcdfDeflateLevelFlag(0)\r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    script = sys.argv[0]\r\n",
        "    u_file = sys.argv[1]\r\n",
        "    u_var = sys.argv[2]\r\n",
        "    v_file = sys.argv[3]\r\n",
        "    v_var = sys.argv[4]\r\n",
        "    outfile_name = sys.argv[5]\r\n",
        "    \r\n",
        "    u_data, ufile_atts = read_data(u_file, u_var)\r\n",
        "    v_data, vfile_atts = read_data(v_file, v_var)\r\n",
        "    \r\n",
        "    wsp_data = calc_wsp(u_data, v_data) \r\n",
        "    \r\n",
        "    write_output(wsp_data, ufile_atts, outfile_name)    \r\n",
        "\r\n",
        "\r\n",
        "def read_data(ifile, var):\r\n",
        "    \"\"\"Read data from ifile corresponding to the var variable\"\"\"\r\n",
        "    \r\n",
        "    fin = cdms2.open(ifile)\r\n",
        "    data = fin(var)\r\n",
        "    file_atts = fin.attributes \r\n",
        "    fin.close()\r\n",
        "    \r\n",
        "    return data, file_atts\r\n",
        "\r\n",
        "\r\n",
        "def calc_wsp(uwnd, vwnd):\r\n",
        "    \"\"\"Calculate the wind speed and create relevant attributes\"\"\"\r\n",
        "    \r\n",
        "    wsp = (uwnd**2 + vwnd**2)**0.5\r\n",
        "    \r\n",
        "    wsp.id = 'wsp'\r\n",
        "    wsp.long_name = 'Wind speed'\r\n",
        "    wsp.units = 'm s-1'\r\n",
        "    \r\n",
        "    return wsp\r\n",
        "\r\n",
        "\r\n",
        "def write_output(wsp_data, ufile_atts, outfile_name):\r\n",
        "    \"\"\"Write the output file\"\"\"\r\n",
        "    \r\n",
        "    outfile = cdms2.open(outfile_name, 'w')\r\n",
        "        \r\n",
        "    new_history = create_history()\r\n",
        "    old_history = ufile_atts['history']\r\n",
        "\r\n",
        "    setattr(outfile, 'history', \"\"\"%s\\n%s\"\"\" %(new_history, old_history))\r\n",
        "    for att_name in ufile_atts.keys():\r\n",
        "        if att_name != \"history\":  # history excluded because we've already done it\r\n",
        "            setattr(outfile, att_name, ufile_atts[att_name])\r\n",
        "\r\n",
        "    outfile.write(wsp_data)\r\n",
        "    outfile.close()\r\n",
        "\r\n",
        "\r\n",
        "def create_history():\r\n",
        "    \"\"\"Create the new entry for the global history file attribute\"\"\"\r\n",
        "    \r\n",
        "    time_stamp = datetime.datetime.now().strftime(\"%a %b %d %H:%M:%S %Y\")\r\n",
        "    exe = sys.executable\r\n",
        "    args = \" \".join(sys.argv)\r\n",
        "    git_hash = Repo(os.getcwd()).head.commit.hexsha\r\n",
        "\r\n",
        "    return \"\"\"%s: %s %s (Git hash: %s)\"\"\" %(time_stamp, exe, args, git_hash[0:7])\r\n",
        "\r\n",
        "\r\n",
        "main()\r\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(The `cdms2.setNetcdf...` commands simply specify that we want the classic netCDF format - see [this post](http://drclimate.wordpress.com/2014/06/09/are-you-cf-compliant/) for more details on netCDF formats.)\n",
      "\n",
      "We can now run this script at the command line:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!/usr/local/uvcdat/1.5.1/bin/python calc_wind_speed.py uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc uas vas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc vas wsp_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The finished product"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now inspect the attributes in our new file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ncdump -h wsp_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "netcdf wsp_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus {\r\n",
        "dimensions:\r\n",
        "\ttime = UNLIMITED ; // (3 currently)\r\n",
        "\tbound = 2 ;\r\n",
        "\tlat = 28 ;\r\n",
        "\tlon = 26 ;\r\n",
        "variables:\r\n",
        "\tdouble time(time) ;\r\n",
        "\t\ttime:bounds = \"time_bnds\" ;\r\n",
        "\t\ttime:units = \"days since 1-01-01 00:00:00\" ;\r\n",
        "\t\ttime:standard_name = \"time\" ;\r\n",
        "\t\ttime:calendar = \"proleptic_gregorian\" ;\r\n",
        "\t\ttime:axis = \"T\" ;\r\n",
        "\tdouble time_bnds(time, bound) ;\r\n",
        "\tdouble lat(lat) ;\r\n",
        "\t\tlat:bounds = \"lat_bnds\" ;\r\n",
        "\t\tlat:units = \"degrees_north\" ;\r\n",
        "\t\tlat:long_name = \"latitude\" ;\r\n",
        "\t\tlat:standard_name = \"latitude\" ;\r\n",
        "\t\tlat:axis = \"Y\" ;\r\n",
        "\tdouble lat_bnds(lat, bound) ;\r\n",
        "\tdouble lon(lon) ;\r\n",
        "\t\tlon:bounds = \"lon_bnds\" ;\r\n",
        "\t\tlon:modulo = 360. ;\r\n",
        "\t\tlon:long_name = \"longitude\" ;\r\n",
        "\t\tlon:standard_name = \"longitude\" ;\r\n",
        "\t\tlon:units = \"degrees_east\" ;\r\n",
        "\t\tlon:axis = \"X\" ;\r\n",
        "\t\tlon:topology = \"circular\" ;\r\n",
        "\tdouble lon_bnds(lon, bound) ;\r\n",
        "\tfloat wsp(time, lat, lon) ;\r\n",
        "\t\twsp:associated_files = \"baseURL: http://cmip-pcmdi.llnl.gov/CMIP5/dataLocation gridspecFile: gridspec_atmos_fx_ACCESS1-3_rcp85_r0i0p0.nc\" ;\r\n",
        "\t\twsp:long_name = \"Wind speed\" ;\r\n",
        "\t\twsp:standard_name = \"eastward_wind\" ;\r\n",
        "\t\twsp:cell_methods = \"time: mean\" ;\r\n",
        "\t\twsp:units = \"m s-1\" ;\r\n",
        "\t\twsp:missing_value = 1.e+20f ;\r\n",
        "\t\twsp:history = \"2012-03-14T04:40:42Z altered by CMOR: Treated scalar dimension: \\'height\\'. 2012-03-14T04:40:42Z altered by CMOR: replaced missing value flag (-1.07374e+09) with standard missing value (1e+20).\" ;\r\n",
        "\r\n",
        "// global attributes:\r\n",
        "\t\t:Conventions = \"CF-1.4\" ;\r\n",
        "\t\t:history = \"Tue Jun 10 17:00:00 2014: /usr/local/uvcdat/1.5.1/bin/python calc_wind_speed.py uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc uas vas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc vas wsp_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc (Git hash: 2db1f92)\\n\",\r\n",
        "\t\t\t\"Thu Nov 07 14:19:44 2013: cdo sellonlatbox,110,160,-45,-10 uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008_aus.nc\\n\",\r\n",
        "\t\t\t\"Thu Nov 07 14:13:51 2013: cdo seldate,2050-06-01,2050-08-31 uas_Amon_ACCESS1-3_rcp85_r1i1p1_200601-210012.nc uas_Amon_ACCESS1-3_rcp85_r1i1p1_205006-205008.nc\\n\",\r\n",
        "\t\t\t\"CMIP5 compliant file produced from raw ACCESS model output using the ACCESS Post-Processor and CMOR2. 2012-03-14T04:40:43Z CMOR rewrote data to comply with CF standards and CMIP5 requirements. Fri Apr 13 12:32:01 2012: corrected model_id from ACCESS1-3 to ACCESS1.3 Fri Apr 13 14:07:50 2012: forcing attribute modified to correct value Wed May  2 13:39:09 2012: updated version number to v20120413.\" ;\r\n",
        "\t\t:initialization_method = 1 ;\r\n",
        "\t\t:CDI = \"Climate Data Interface version 1.5.6 (http://code.zmaw.de/projects/cdi)\" ;\r\n",
        "\t\t:product = \"output\" ;\r\n",
        "\t\t:creation_date = \"2012-03-14T04:40:43Z\" ;\r\n",
        "\t\t:frequency = \"mon\" ;\r\n",
        "\t\t:references = \"See http://wiki.csiro.au/confluence/display/ACCESS/ACCESS+Publications\" ;\r\n",
        "\t\t:title = \"ACCESS1-3 model output prepared for CMIP5 RCP8.5\" ;\r\n",
        "\t\t:experiment = \"RCP8.5\" ;\r\n",
        "\t\t:realization = 1 ;\r\n",
        "\t\t:project_id = \"CMIP5\" ;\r\n",
        "\t\t:institute_id = \"CSIRO-BOM\" ;\r\n",
        "\t\t:model_id = \"ACCESS1.3\" ;\r\n",
        "\t\t:parent_experiment_id = \"historical\" ;\r\n",
        "\t\t:experiment_id = \"rcp85\" ;\r\n",
        "\t\t:cmor_version = \"2.8.0\" ;\r\n",
        "\t\t:parent_experiment = \"historical\" ;\r\n",
        "\t\t:modeling_realm = \"atmos\" ;\r\n",
        "\t\t:branch_time = 732311. ;\r\n",
        "\t\t:institution = \"CSIRO (Commonwealth Scientific and Industrial Research Organisation, Australia), and BOM (Bureau of Meteorology, Australia)\" ;\r\n",
        "\t\t:version_number = \"v20120413\" ;\r\n",
        "\t\t:forcing = \"GHG, Oz, SA, Sl, Vl, BC, OC, (GHG = CO2, N2O, CH4, CFC11, CFC12, CFC113, HCFC22, HFC125, HFC134a)\" ;\r\n",
        "\t\t:CDO = \"Climate Data Operators version 1.5.6.1 (http://code.zmaw.de/projects/cdo)\" ;\r\n",
        "\t\t:physics_version = 1 ;\r\n",
        "\t\t:contact = \"The ACCESS wiki: http://wiki.csiro.au/confluence/display/ACCESS/Home. Contact Tony.Hirst@csiro.au regarding the ACCESS coupled climate model. Contact Peter.Uhe@csiro.au regarding ACCESS coupled climate model CMIP5 datasets.\" ;\r\n",
        "\t\t:table_id = \"Table Amon (01 February 2012) 01388cb4507c2f05326b711b09604e7e\" ;\r\n",
        "\t\t:tracking_id = \"724f536a-c5fa-4a68-85f1-ff277af34c75\" ;\r\n",
        "\t\t:parent_experiment_rip = \"r1i1p1\" ;\r\n",
        "}\r\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since most of the file attributes were inherited by default from the input data file (i.e. the u-wind file), it's worth checking to see if there are any that don't make sense. Sure enough, the standard name is misleading:\n",
      "\n",
      "`wsp:standard_name = \"eastward_wind\"`\n",
      "\n",
      "We should revise our script so that it removes or renames the standard name, but beyond that we should resist the urge to start cleaning up. The `associated_files` wind speed attribute, for instance, makes little sense to anyone who isn't involved in the CMIP5 project. While this might seem like a reasonable argument for deleting that attribute, once an attribute is deleted it's gone forever. The `associated_files` attribute is taking up a negligible amount of memory, so why not just leave it there just in case? When in doubt, keep metadata."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Challenge\n",
      "\n",
      "Does your data management plan from the first challenge adequately address this issue of data provenance? If not, go ahead and add to your plan now. Things to consider include:\n",
      "\n",
      "* How to capture and store metadata relating to output files that aren't self describing (i.e. unlike `.nc` files, formats like `.csv` or `.png` don't store things like global and variable attributes within them) \n",
      "\n",
      "Discuss the additions you've made to your plan with your partner."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}