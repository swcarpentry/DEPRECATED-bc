The Shell
=========

> This file contains instructor's notes for a 3-4 hour shell session 
> on the shell for beginners.  It requires the "North Pacific Gyre" directory 
> with associated data files from the original SWC bootcamp repository.  Most 
> of this document reads like a script, with what to say and which commands to use/demonstrate 
> in the shell.  Student exercises and other instructor notes are indicated 
> by blockquotes within the document.  

##Introduction

We're here to teach you about computers and computers really do four things:

* run programs
* store data 
* communicate with each other
* and interact with us

They can do the last of these in many different ways.
The most common way of doing this is using a graphical interface.
Another way to do this is using a text-based command line interface

To interact with our computers using the command line we use something called a
command shell, which is a program whose job is to run other programs. We're
going to be using the Bash shell, which is the default shell on Mac and
Linux. Bash stands for Bourne Again SHell because it was written by Stephen
Bourne.

The shell is very useful to scientists for three main reasons:

1. Automating data analysis and data management
2. Combining existing tools in powerful ways with only a few keystrokes
3. Interacting with remote machines

I'll give you two quick examples of how we regularly use the shell in my group:

1. Who here has some aspect of their research that involves a bunch of different
   files, or a bunch of different tabs in an spreadsheet that all hold the same
   kind of information and need to be processed in similar ways? We often have a
   number of different data files that represent data from a number of different
   studies, or sites, or times. We typically need to run the same analysis on
   all of these different files and that analysis often involves combining code
   written in both Python and R, and sometimes using programs written by other
   people in other languages (e.g., like MARK for mark-recapture analyses). The
   things we're going to learn this morning make it easy to do this in a fully
   automated way, so we don't have to spend days or even weeks repeatedly
   running analyses by hand.
2. Who has done some data analysis or simulation modeling recently that took a
   long time to run and your computer was horribly slow while it was running? We
   also do simulation modeling, which requires running the same basic code with
   different sets of parameters and then combining the results. Since these
   simulations take a long time to run we typically run them on a university
   cluster so that our desktops and laptops aren't grindingly slow for days
   while the analysis runs. The things we'll learn this morning make automating
   this process and running it on someone elses computer much easier.

Some of the things we'll learn this morning can be accomplished in R or Python,
but in many cases this requires understanding how to do them in the shell.  

**Instructor**
>Open a terminal;
>
>>Windows open GitBash;
>
>>Mac type 'terminal' into the Finder;

>Check that everyone has stickies

##Files and Directories

We'll start our shell lesson by exploring how we can use it to accomplish 
the second item listed above - storing data.  Storing your data intelligently - 
being able to navigate directories and manipulate data files - is an important 
starting point to streamlining your research.  After this portion 
of the lesson, you should be able to use the shell to:

* Find where you are in your file system, move around, and see what files you have.
* Make new files and directories.
* Copy, move, and delete files and directories.

### Viewing the file system

**Instructor**
> Tab-completion is discussed after doing the first round of commands.  Try to 
> refrain from tab-completion here, both to avoid potential confusion and to keep yourself 
> from zipping along too fast at the beginning.  

Let's see where we are.

    pwd

This stands for "print working directory" and tells us where we are in the file
system. Your terminal probably started in your home directory like mine
did. Specifically this output tells us that I'm in a directory named `ethan`
that is a subdirectory of a directory named `home`. Windows users will probably
see a letter `c` at the beginning.

`pwd` is actually a program living in your computer.  When we run `pwd` in the shell, it works by:

1. finding a program called `pwd`,
2. running that program,
3. and displaying that program's output,
4. and then displaying a new prompt to let us know that it's ready for more input

To take a look at what's in the current directory use

    ls

which stands for "listing". This will print out a list of files and directories
just like using the graphical file browser on your computer and we can interact
with files and directories/folder in the usual ways using the command line.

In some cases the directories will be colored but if not we can see which names
are directories and which are files by using the -F argument

    ls -F
	
which adds a trailing slash to the directories.  Dash is how we pass options to
commands, so this says "run ls in such a way that it shows me the folders".

*Instructor*
> Start a diagram (on the board and/or have students write themselves) of a typical shell command: 
> program (first word) + options (indicated by a dash)

### Changing directories

To change directories

    cd Desktop
	
Since `Desktop` is in our current directory we can just type it's name.

If we now run

    pwd

and

    ls

We see that we are in the Desktop folder and that it is the same as our
computer's desktop. In other words, you're Desktop is just a special folder in
your home directory.

We can also specify the full path to allow us to jump anywhere.

    cd /home/ethan/
	
Windows users may need to start full paths with the drive letter.

    cd /c/Users/username

If you want to move up one directory we can use

    cd ..

#### Example

To help us learn the shell we're going to use an example that captures some of
the common situations we run into when analyzing data.  We'll introduce the 
data now, go back and learn some useful shell commands, and then 
revisit this example with some more tools in our tool kit.  

Nelle is a marine biologist. She recently returned from a survey collecting data
on gelatinous marine life. She has 250 samples and has assayed them all to
determine the relative abundance of 300 different proteins. Each assay results
in a single file with one line for each protein.

To analyze this data she needs to:
1. Calculate statistics for each of the proteins separately using a program her
   supervisor wrote called `goostat`.
2. Compare the statistics for each protein with corresponding statistics for
   each other protein using a program one of the other graduate students wrote
   called `goodiff`.

Running the `goostat` program 250 times would be time consuming, but
possible. Running the nearly 60K pairwise comparisons one at a time would take
at least several hundred hours.

**Student Exercise**
>Let's practice what we've learned so far by looking at Nelle's data. Download
  and unzip the file on your Desktop. Use the shell to look at what is in the
  north-pacific-gyre directory and any subdirectories that it contains.

**Instructor**
> Ask learners: What did you find?  

Reminder: you don't have to `cd` into a directory just to see what
is there; you can always use `ls` with the directory name.

    ls 2012-07-03

**Student Exercise**
> If you're in the North Pacific Gyre directory, what are (at least) three different ways to see the contents 
of the directory Creatures (or comparable)?  

**Instructor**
> Go over previous exercise: use absolute path or relative path with `ls` or `cd` into creatures and then use `ls`.  

> Complete the diagram of a typical shell command: program + options + arguments, which 
> can be names of files or path names


#### Naming

Now Nelle's done a really nice job here with something. What do you think it is?

That's right, she's used names that contain a lot of information. And this is

| Rule #1        |
| ------------- |
| Always use meaningful, consistently structured, names.  | 

Her naming conventions do two things. First they help chunk information for
us. By looking at the names of the directories we know what they
contain. And in fact each of the file names contains her labs unique code for
the location, time, depth, and other characteristics of the sample.

Second, using consistently structured file names makes it easier to 
process the data automatically, because it will be easy to identify the data files we want.

#### Autocomplete / Up & Down Arrows

Before we move on to the rest of our data management tools, let's discuss a 
few shortcuts that will allow you to get things done faster.  For example, 
these informative names are great, but does it seem to anyone else like they
require a lot of typing.  Instead of typing in full names, you can use the 
tab key to complete them for you.  

    cd /h<tab>/e<tab>/D<tab>
	
If nothing happens then either there is nothing to complete or there is too
much. A second ``<tab>`` will show you the available options.

You can also use the up and down arrows to cycle through commands and edit only
the part of a command that you want to change.

|Rule #2|
|--------|
|If the computer can do it, let the computer do it.|

**Student Exercise?**

### Making directories

We can add new directories using

    mkdir thesis

Running

    ls

shows us that thesis exists, and

    ls thesis

shows us there's nothing in it.  Let's move into this directory and make some files.  

	cd thesis

### Making files

To make files in the shell we use simple text editors.
I'm going to use `nano` today since it's one of the most basic and since it should
work for everyone.   

To make a new file we simply type ``nano`` and then the name of the file.
Let's make a rough outline of our thesis:

    nano outline.txt

Type in a few lines of text:

```
1. Collect data
2. ?
3. Profit
```

then use Control-O to save our data (the O stands for out, the ^ in most Unix
documentation means Control).

Control-X quits the editor and return to the shell. `nano` doesn't leave any
output on the screen after it exits, but

    ls

now shows that we have created a file called `outline.txt`.
	
You can also use your favorite text editor for this, just make sure to save your
file in the right directory.

**Student Exercise**
> Make an additional document in the thesis folder called `references.txt`.  

**RED LIGHT/GREEN LIGHT**

### Copying, Moving, Renaming, and Deleting Files

We can make copies of files using ``cp`` for copy

    cp outline.txt copy_of_outline.txt

where first parameter tells `cp` what we're copying, and the second is where to
put the copy and what to call it.
	
	ls

shows us that we've made a copy of the file in the thesis directory.

And we can move files using ``mv``

    mkdir temp
	ls
	mv copy_of_outline.txt temp
	ls
	cd temp
	ls

We can also use mv to rename files using ``mv`` by "moving" them to a new
file name. Maybe we're feeling enthusiastic about the state of our outline so we
decide to rename it.

    mv outline.txt rough_draft.txt
	ls

If we want to get rid of files we use `rm` for remove

    rm rough_draft.txt

**Student Exercise**
>Create a backup copy of your thesis, you do keep backups of your data right?;

> Make a new outline.txt file (or rename copy_of_outline.txt);

> Make a directory called backup inside your thesis directory;

> Make a copy of your outline.txt file;

> Move it into the backup folder;

> Change the backup file's name to include todays date**

This sums up our data management tools.  We'll take a break now.  
After the break, we'll start looking at the other major task of computers - 
running programs - and how we can combine and write our own using shell commands.  

**Instructor**
> Break

***

## Elements of Automation

In this portion of the lesson, we'll move away from commands that manage our files and 
directories and dive in to running programs and putting programs together.  In 
particular, we'll discuss all the different tools you will probably use when 
automating a process by writing a script.  After this lesson, you should be able to

* Use built-in shell program(s).
* Understand how to use the * wildcard to process multiple files.
* Use the two different command line channels: redirects and pipes.
* Describe how a for loop works.
* Describe at least one benefit/reason to write a script.  

We'll return to our friend Nelle to discuss these topics.  

### Using a built-in program

Now that we know how to work with the shell we can use it to do powerful things
by combining lots of small built in programs. 

Nelle's assays have now finished running, so she can get back to working with
the data itself.

The first thing she wants to do is run some basic sanity checks on the
data. Remember that each assay is supposed to include measures of 300 different
proteins, one per line, so Nelle wants to look to see if any files have too many
or too few lines.

She can find out the number of lines in a file using the command `wc` for "word count".

    wc NENE01729A.txt

This actually shows us the number of characters, words, and lines in the file,
so we can add a `-l` for "lines" to tell `wc` to just show the line counts.

    wc -l NENE01729A.txt

That's great for one file, but we need the counts for all the files. `wc` will
take multiple file names as input

    wc -l NENE01729A.txt NENE01729B.txt

but that's a lot of typing even with the tab key.

### Wildcards

We can use wildcards to work with multiple files at once. So

    wc -l *.txt

counts the lines for all of the files ending in .txt. The shell expands the
wildcard before executing `wc`, so this is identical to typing out all of the
filenames.

### Redirects

With a small number of files we could probably just look through the counts to
see if there were any problems, but with hundreds or thousands of files that
won't work very well, so let's store this output so that we can work with it
further.

    wc -l *.txt > lengths

The `>` sign redirects the output of the `wc` command into a file called
lengths, instead of printing it to the screen.

We can look at the contents of `lengths` using

    cat lengths

`cat` stands for "concatenate": it prints the contents of files one after
another. There's only one file in this case, so cat just shows us what it
contains.

Now let's use the `sort` command to sort its contents.

    sort lengths

And we can save that to a file called `sorted-lengths`

    sort lengths > sorted-lengths

Finally we can look at just the last few lines of our sorted file to see if
there are any files with too many lines.

    tail sorted-lengths

**Student Exercise**
> Perform the same basic santity check, but only for the files that end in A.txt
> and use a command called head, which does the opposite of tail, to show the
> files with the fewest rows.  What did you see? Any problems

When Nelle goes back and checks it, she sees that she did that assay at 8:00 on a
Monday morning—someone was probably in using the machine on the weekend, and she
forgot to reset it.

### Pipes

We can also skip the intermediate files by using pipes.
Pipes pass the data that would have gone into the intermediate program
straight to the next program.

So, the commands we just wrote:

    sort lengths > sorted-lengths
    tail sorted-lengths

can be written like this:

    sort lengths | tail

The output from the first command is passed through the pipe to the second
command which takes it was input. This simple idea is why Unix has been so
successful. Instead of creating enormous programs that try to do many different
things, Unix programmers focus on creating lots of simple tools that each do one
job well, and that work well with each other. This programming model is called
pipes and filters. We've already seen pipes; a filter is a program like wc or
sort that transforms a stream of input into a stream of output.


### Other people's programs

Piping together unix tools can be very powerful for automating data analysis.  I
often use this for quick sanity checks on much more complicated Python code. But
the real power of the piping in my every day life is piping my tools and other
scientists tools together. This makes it easy to use tools written in languages
you're not familiar with and to combine tools from different languages.

|Rule #3 |
|--------|
| Don't reinvent the wheel.|

So, recall that Nelle needs to calculate some statistics for each of her
datasets. She could figure out how to do this herself, but one of her lab mates
already wrote code to do this for another project. Unfortunately the code is
written in Python and Nelle is only familiar with R, but that's OK, because she
can make it work using the shell.

The file is called `goostats.py`. To run this code we need to tell the shell to
run it using python, which we do by giving it the name of the program that will
run it, then the name of our program, and then the input.

    python goostats.py NENE01729A.txt
	
This can then be integrated into our pipeline. So if we want to sort based
on the total number of individuals:
	
    python goostats.py NENE01729A.txt | head -1
	
**Student Exercise**
>Using pipes run goostats on only the data from the first 50 rows of 
>NENE01729A.txt and store the results to a file with a name indicating the 
>analysis that the file contains.  Then make a directory called analysis and move 
>the file into that directory.  

**Instructor**
> Break

 ***

### Loops

This is great for a single datafile but Nelle has a lot of datafiles and needs
to analyze them all at once. `goostats` is only designed to handle one file at a
time, so to do this we need to use a for loop. This loop will do some operation
once for each thing in a list. In this case the list of things is a list of
filenames, so we'll start by figuring out how to just print that list of file
names.

    for datafile in *.txt
	do
	    echo $datafile
	done

When the shell sees the keyword `for`, it knows it is supposed to repeat a command
(or group of commands) once for each thing in a list. In this case, the list is
the two filenames. Each time through the loop, the name of the thing currently
being operated on is assigned to the variable called filename. Inside the loop,
we get the variable's value by putting `$` in front of it: `$filename` is
NENE01729A.txt the first time through the loop, NENE01729B.txt the second, and so
on. Finally, the command that's actually being run tells the shell to print, so
this loop prints out the name of each data file in turn.

There's nothing special about the word `datafile`; we could use any variable name we 
want, although its usually a good idea to stay away from funny characters.  
Rule #1 applies here also; it's a good idea to use variable names that have some 
meaning within the program.  

Our for loop can include as many commands as we want

    for datafile in *.txt
	do
	    echo $datafile
		head $datafile
	done

It's often a good idea to print commands (using `echo`) before actually 
running them in a loop, to make sure the loop is doing what you think it is.  
	
**Student Exercise**
> Write a for loop that, for each datafile, prints the name of the file and then
runs goostats on it.

Maybe we don't just want to run `goostats`, we want to run it and save the data 
in another file, one output file for each data file.  To do this, we just need to 
add a redirect to the program we've already written.  

    for datafile in *.txt
	do
        python goostats.py $datafile > stats-$datafile
	done

And so now we have all of the output of our statistics runs in a well named and
structured format that is ready for further analysis.

## Putting it all together

### Bash scripting

Once we have the commands that we want working we typically want to store them
since if we needed to do it once, we'll probably need to do it again.  To do
this easily we can store them in a shell script. We do this by adding the
commands to a text file and then running that text file from the command line.

Who remembers all of the commands we used exactly to get to this point?  
If you don't remember, you can use the `history` command to tell you.  (Hint: 
it's a good idea to pipe this command to tail, as your history may be very 
very long if you've been working for a couple of hours)

Recall from the first session - how do you create a new file?  

	nano do-stats.sh
	
In your newly opened file, put in the shell code we've just developed.  

    for datafile in *.txt
	do
        python goostats.py $datafile > stats-$datafile
	done

To run your new script, you'll need one more command.  

    bash do-stats.sh

Now, as Nelle finishes up more assays she can simply add the datafiles to her
directory and rerun this function and automatically update her analysis. In
addition, if Nelle or her advisor ever change their minds about the details of
the analysis, they can make a simple change and then rerun everything.

**Instructor**
> What's missing?  Fish until you get "it's missing documentation."  

###Documentation

|Rule #4  |
----------
|Document the purpose of your code| 

This will make it much easier for other people (especially your future self) to re-use the code.  

**Student Exercise**
> Modify your shell script so that the output only includes the first line of
output from goostats, the all important 'goo-coefficient'

> Add some comments explaining what this code does.  

###Arguments

Let's add one more thing to our script before we're done. Often we don't want to
store our raw data and the output from our analyses in the same directory.

So let's design our bash script so that it can take an argument that tells it where
we want to store the output.

To do this we use a special variable `$1`. Inside a shell script, $1 means "the
first filename (or other parameter) on the command line".
	
    mkdir $1
    for datafile in *.txt
	do
        python goostats.py $datafile > $1/stats-$datafile
	done

We can now run our script like this:

    bash do-stats.sh goostats-results

We've built this script up piece by piece, and in fact that's exactly how most
people work and in fact I find that it is much more efficient to do something
small, make sure that it's working, and then do the next part, then to try to
do everything at once and then spend hours tracking down all of the bugs.
